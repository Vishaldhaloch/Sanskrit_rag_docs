# generator.py
from llama_cpp import Llama
import os

MODEL_PATH = r"E:\RAG_Sanskrit_Vishal_dhaloch_code\code\models\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"

class LlamaGenerator:
    def __init__(self):
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"Model not found: {MODEL_PATH}")

        self.llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=1024,
            n_threads=1,     # üî¥ keep 1
            n_batch=1,       # üî¥ keep 1
            use_mlock=False,
            use_mmap=True,
            verbose=False
        )


    def generate(self, context, question):
    # üî¥ HARD truncate context (token-safe approximation)
        context = context[:400]

        prompt = (
            "‡§â‡§§‡•ç‡§§‡§∞‡§Ç ‡§ï‡•á‡§µ‡§≤‡§Ç ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡•á‡§® ‡§≤‡§ø‡§ñ‡§§‡•§\n\n"
            "‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§É:\n"
            f"{context}\n\n"
            "‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§É:\n"
            f"{question}\n\n"
            "‡§â‡§§‡•ç‡§§‡§∞‡§É:\n"
        )

        out = self.llm(prompt, max_tokens=64)  # üî¥ reduce
        return out["choices"][0]["text"].strip()



